{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T04:56:24.868668Z","iopub.execute_input":"2025-11-26T04:56:24.868997Z","iopub.status.idle":"2025-11-26T04:56:25.241272Z","shell.execute_reply.started":"2025-11-26T04:56:24.868974Z","shell.execute_reply":"2025-11-26T04:56:25.240395Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" # üéì Capstone Project: Adaptive Spaced Repetition Tutor (SRT)\n ## Phase 1: The Agentic Memory Foundation","metadata":{}},{"cell_type":"markdown","source":"**Project Overview:**\n\nThis agent solves the \"forgetting curve\" problem for students. It ingests study materials (summarized by NotebookLM), consolidates them into atomic facts, and schedules them for adaptive review.","metadata":{}},{"cell_type":"markdown","source":"**Phase 1 Goals:**\n1.  Setup the ADK environment.\n2.  Define the `KnowledgeFact` data structure.\n3.  Build the `MemoryConsolidationAgent` to extract facts from raw text.\n4.  Initialize the `MemoryService` to store these facts.","metadata":{}},{"cell_type":"markdown","source":"## ‚öôÔ∏è Section 1: Setup & Configuration\n\nFirst, we install the Google Agent Development Kit (ADK) and configure our secure access credentials.","metadata":{}},{"cell_type":"code","source":"# 1.1 Install ADK\n\n!pip install google-adk pydantic --quiet\nprint(\"‚úÖ ADK installed successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T04:56:25.242623Z","iopub.execute_input":"2025-11-26T04:56:25.243060Z","iopub.status.idle":"2025-11-26T04:56:29.985784Z","shell.execute_reply.started":"2025-11-26T04:56:25.243037Z","shell.execute_reply":"2025-11-26T04:56:29.984840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1.2 Configure API Key (Securely)\n\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\ntry:\n    # Retrieve the API key from Kaggle Secrets\n    GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n    print(\"‚úÖ Gemini API key setup complete.\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Authentication Warning: {e}\")\n    print(\"If running locally, ensure GOOGLE_API_KEY is set in your environment.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T04:56:29.987028Z","iopub.execute_input":"2025-11-26T04:56:29.987439Z","iopub.status.idle":"2025-11-26T04:56:30.137423Z","shell.execute_reply.started":"2025-11-26T04:56:29.987403Z","shell.execute_reply":"2025-11-26T04:56:30.136632Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1.3 Import ADK Components\n\nfrom google.adk.agents import LlmAgent\nfrom google.adk.models.google_llm import Gemini\nfrom google.adk.runners import Runner\nfrom google.adk.sessions import InMemorySessionService\nfrom google.adk.memory import InMemoryMemoryService\nfrom google.adk.tools import ToolContext\nfrom google.genai import types\nfrom typing import List, Dict, Any\nfrom pydantic import BaseModel, Field\nimport json\nimport datetime\n\nprint(\"‚úÖ Imports complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T04:56:30.138203Z","iopub.execute_input":"2025-11-26T04:56:30.138507Z","iopub.status.idle":"2025-11-26T04:56:50.407529Z","shell.execute_reply.started":"2025-11-26T04:56:30.138484Z","shell.execute_reply":"2025-11-26T04:56:50.406755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1.4 Configuration & Best Practices\n# We use a robust retry configuration to handle potential API timeouts/errors.\nretry_config = types.HttpRetryOptions(\n    attempts=5,\n    exp_base=7,\n    initial_delay=1,\n    http_status_codes=[429, 500, 503, 504],\n)\n\n# Model Configuration\nMODEL_NAME = \"gemini-2.5-flash-lite\" # Cost-effective and fast for extraction\nAPP_NAME = \"SRT_Tutor_App\"\nUSER_ID = \"student_user\"\n\nprint(\"‚úÖ Configuration ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T04:56:50.409210Z","iopub.execute_input":"2025-11-26T04:56:50.409679Z","iopub.status.idle":"2025-11-26T04:56:50.415373Z","shell.execute_reply.started":"2025-11-26T04:56:50.409656Z","shell.execute_reply":"2025-11-26T04:56:50.414663Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üß† Section 2: Defining the \"Knowledge\" Structure\n\nUnlike a simple chat bot, our tutor needs structured data. We use `Pydantic` to define exactly what a \"Memory Fact\" looks like. This ensures our agent produces consistent data we can query later.","metadata":{}},{"cell_type":"code","source":"class KnowledgeFact(BaseModel):\n    \"\"\"Schema for a single unit of knowledge to be memorized.\"\"\"\n    topic: str = Field(..., description=\"The broad subject (e.g., 'Deep Learning', 'Python').\")\n    concept: str = Field(..., description=\"The specific concept name (e.g., 'Dropout', 'List Comprehension').\")\n    definition: str = Field(..., description=\"A concise, one-sentence definition or explanation.\")\n    confidence: str = Field(\"High\", description=\"Current confidence level: High, Medium, or Low.\")\n    last_reviewed: str = Field(..., description=\"Date string YYYY-MM-DD.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T04:56:50.416067Z","iopub.execute_input":"2025-11-26T04:56:50.417048Z","iopub.status.idle":"2025-11-26T04:56:50.438893Z","shell.execute_reply.started":"2025-11-26T04:56:50.417025Z","shell.execute_reply":"2025-11-26T04:56:50.437907Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üõ†Ô∏è Section 3: Custom Tools for Memory Ingestion\n\nWe need a custom tool that the Agent can call to \"save\" what it learns. This tool takes the extracted facts and puts them into our `MemoryService`.\n","metadata":{}},{"cell_type":"code","source":"# Initialize the Memory Service (Simulating Long-Term Persistence)\n# In production, you might use VertexAiMemoryBankService, but InMemory is perfect for the Capstone demo.\n\nmemory_service = InMemoryMemoryService()\n\ndef save_knowledge_facts(tool_context: ToolContext, facts: List[Dict[str, Any]]) -> str:\n    \"\"\"\n    Saves a list of extracted knowledge facts into the user's long-term memory.\n    \n    Args:\n        facts: A list of dictionaries, where each dictionary represents a KnowledgeFact \n               (topic, concept, definition, confidence, last_reviewed).\n    \"\"\"\n    print(f\"\\nüíæ TOOL CALL: Saving {len(facts)} new facts to memory...\")\n    \n    for fact_data in facts:\n        # In a real app, we might use memory_service.add_memory(fact) \n        # Here, we simulate it by storing it in the session state or a global store\n        # for immediate retrieval demonstration.\n        \n        # We can leverage the tool_context state to persist this for the session\n        if \"knowledge_base\" not in tool_context.state:\n            tool_context.state[\"knowledge_base\"] = []\n            \n        tool_context.state[\"knowledge_base\"].append(fact_data)\n        print(f\"   - Saved: {fact_data['concept']} ({fact_data['topic']})\")\n        \n    return f\"Successfully saved {len(facts)} facts to long-term memory.\"\n\nprint(\"‚úÖ Custom Tool `save_knowledge_facts` defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T04:56:50.439883Z","iopub.execute_input":"2025-11-26T04:56:50.440260Z","iopub.status.idle":"2025-11-26T04:56:50.454587Z","shell.execute_reply.started":"2025-11-26T04:56:50.440238Z","shell.execute_reply":"2025-11-26T04:56:50.453714Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ü§ñ Section 4: The Memory Consolidation Agent\n\nThis agent acts as the \"Librarian.\" It takes raw, unstructured text (like your NotebookLM summary) and uses the LLM to structure it into the `KnowledgeFact` format before calling the save tool.\n","metadata":{}},{"cell_type":"code","source":"consolidation_agent = LlmAgent(\n    name=\"MemoryConsolidator\",\n    model=Gemini(model=MODEL_NAME, retry_options=retry_config),\n    instruction=f\"\"\"\n    You are an expert Knowledge Consolidator. Your goal is to help a student memorize complex topics.\n    \n    INPUT: You will receive raw study notes or summaries.\n    \n    TASK:\n    1. Analyze the text and extract key concepts, definitions, and facts.\n    2. Transform these into a structured list of facts.\n    3. For each fact, assign a 'topic', 'concept', and a clear 'definition'.\n    4. Set 'confidence' to 'High' (default) and 'last_reviewed' to today's date.\n    5. You MUST use the `save_knowledge_facts` tool to store these facts.\n    \n    Do not just reply with text. You must CALL THE TOOL to save the data.\n    \"\"\",\n    tools=[save_knowledge_facts]\n)\n\nprint(\"‚úÖ Consolidation Agent initialized.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T04:56:50.455487Z","iopub.execute_input":"2025-11-26T04:56:50.455802Z","iopub.status.idle":"2025-11-26T04:56:50.471334Z","shell.execute_reply.started":"2025-11-26T04:56:50.455772Z","shell.execute_reply":"2025-11-26T04:56:50.470422Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üöÄ Section 5: Running Phase 1\n \nLet's simulate the workflow. Imagine you just finished studying \"Gradient Descent\" and you have a summary from NotebookLM. You paste it here, and the Agent ingests it.\n","metadata":{}},{"cell_type":"code","source":"# 1. Setup the Runner\n# We use InMemorySessionService for the conversation flow\nsession_service = InMemorySessionService()\nrunner = Runner(\n    agent=consolidation_agent,\n    app_name=APP_NAME,\n    session_service=session_service,\n    memory_service=memory_service\n)\n\n# 2. The Input (Simulated output from NotebookLM)\nraw_study_notes = \"\"\"\nSummary of Deep Learning Optimization:\nGradient Descent is an optimization algorithm used to minimize some function by iteratively moving in the \ndirection of steepest descent as defined by the negative of the gradient.\nThe Learning Rate is a hyperparameter that controls how much to change the model in response to the \nestimated error each time the model weights are updated. If it's too small, training is slow. If too large, \nit might overshoot the minimum.\nStochastic Gradient Descent (SGD) performs a parameter update for each training example, unlike Batch GD \nwhich uses the whole dataset.\n\"\"\"\n\n# 3. Run the Agent\nprint(f\"user > Processing study notes... (Length: {len(raw_study_notes)} chars)\\n\")\n\n# --- FIX: Create the session explicitly first ---\nsession_id = \"ingestion_session_01\"\nawait session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=session_id)\n\n# We use a loop to process the stream, matching the assignment notebook style\nasync for event in runner.run_async(\n    user_id=USER_ID,\n    session_id=session_id,\n    new_message=types.Content(role=\"user\", parts=[types.Part(text=raw_study_notes)])\n):\n    # We print the tool calls and model responses as they happen\n    if event.content and event.content.parts:\n        for part in event.content.parts:\n            if part.text:\n                print(f\"agent > {part.text}\")\n            if part.function_call:\n                print(f\"üõ†Ô∏è Agent is calling tool: {part.function_call.name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T04:58:35.848417Z","iopub.execute_input":"2025-11-26T04:58:35.848762Z","iopub.status.idle":"2025-11-26T04:58:38.368876Z","shell.execute_reply.started":"2025-11-26T04:58:35.848739Z","shell.execute_reply":"2025-11-26T04:58:38.368139Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ‚úÖ Phase 1 Complete\n \n If you see the \"TOOL CALL: Saving...\" output above, your agent has successfully:\n 1.  **Reasoned** over raw text.\n 2.  **Structured** it into specific data formats.\n 3.  **Executed** a custom tool to persist that knowledge.\n \n This completes the foundation. In Phase 2, we will build the **Assessment Agent** that retrieves these facts to quiz you!","metadata":{}}]}